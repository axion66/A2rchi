# Basic configuration file for a Archi deployment
# with a chat app interface, a QA pipeline, and
# PostgreSQL with pgvector for document storage.
# The LLM is a locally hosted VLLM instance on GPUs.
#
# run with:
# archi create --name my-archi --config examples/deployments/basic-gpu/config.yaml --services chatbot --gpu-ids=all

name: my_archi

services:
  chat_app:
    trained_on: "My data"
  vectorstore:
    backend: postgres  # PostgreSQL with pgvector (only supported backend)

data_manager:
  sources:
    links:
      input_lists:
        - examples/deployments/basic-gpu/miscellanea.list
  embedding_name: HuggingFaceEmbeddings

archi:
  pipelines:
    - QAPipeline
  pipeline_map:
    QAPipeline:
      prompts:
        required:
          condense_prompt: examples/deployments/basic-gpu/condense.prompt
          chat_prompt: examples/deployments/basic-gpu/qa.prompt
      models:
        required:
          chat_model: VLLM
          condense_model: VLLM
  model_class_map:
    VLLM:
      kwargs:
        base_model: 'Qwen/Qwen2.5-7B-Instruct-1M'
        quantization: True
        max_model_len: 32768
        tensor_parallel_size: 2
        repetition_penalty: 1.0
        gpu_memory_utilization: 0.5