# Basic configuration file for a Archi deployment
# with a chat app interface, a QA pipeline, and
# PostgreSQL with pgvector for document storage.
# The LLM is a locally hosted VLLM instance on GPUs.
#
# run with:
# archi create --name my-archi --config examples/deployments/teaching_assistant/config.yaml --podman --services chatbot --gpu-ids=all --agents examples/agents

name: my_archi

services:
  chat_app:
    agent_class: QAPipeline
    agents_dir: examples/agents
    provider: local
    model: Qwen/Qwen2.5-7B-Instruct-1M
    providers:
      local:
        enabled: true
        base_url: http://localhost:8000/v1  # VLLM/OpenAI-compatible endpoint
        mode: openai_compat
        default_model: "Qwen/Qwen2.5-7B-Instruct-1M"
        models:
          - "Qwen/Qwen2.5-7B-Instruct-1M"
    prompts:
      condense_prompt: examples/deployments/teaching_assistant/condense.prompt
      chat_prompt: examples/deployments/teaching_assistant/teaching_assistant.prompt
    trained_on: "My data"
  vectorstore:
    backend: postgres  # PostgreSQL with pgvector (only supported backend)
   
data_manager:
  sources:
    links:
      input_lists:
        - examples/deployments/teaching_assistant/course_materials.list
  embedding_name: HuggingFaceEmbeddings
